{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d054b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fb9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651d71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9bcb0",
   "metadata": {},
   "source": [
    "##todo:\n",
    "splitat u validation i u train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ab8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "1\n",
      "NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd5ae47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transform (Mel spectrogram transformation)\n",
    "transform = MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=400,        # 25 ms window\n",
    "    hop_length=160,   # 10 ms stride\n",
    "    n_mels=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905dac8",
   "metadata": {},
   "source": [
    "ako treba If you still need nn.Sequential later, wrap it like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21c418",
   "metadata": {},
   "source": [
    "transform = nn.Sequential(\n",
    "    MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c99ed92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simple character map (extend as needed)\n",
    "char_map = {c: i+1 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz '\")}\n",
    "char_map['<blank>'] = 0\n",
    "output_size = len(char_map)\n",
    "def text_to_int_sequence(text):\n",
    "    return [char_map.get(c, 0) for c in text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87d4640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyctcdecode\n",
      "  Obtaining dependency information for pyctcdecode from https://files.pythonhosted.org/packages/a5/8a/93e2118411ae5e861d4f4ce65578c62e85d0f1d9cb389bd63bd57130604e/pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from pyctcdecode) (1.26.0)\n",
      "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
      "  Obtaining dependency information for pygtrie<3.0,>=2.1 from https://files.pythonhosted.org/packages/ec/cd/bd196b2cf014afb1009de8b0f05ecd54011d881944e62763f3c1b1e8ef37/pygtrie-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
      "  Obtaining dependency information for hypothesis<7,>=6.14 from https://files.pythonhosted.org/packages/62/66/fe2688be5d80ec57cb7011ac5e5626c27a6c703e6d69515b3f651f8074e1/hypothesis-6.136.2-py3-none-any.whl.metadata\n",
      "  Downloading hypothesis-6.136.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting attrs>=22.2.0 (from hypothesis<7,>=6.14->pyctcdecode)\n",
      "  Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.2.2)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7,>=6.14->pyctcdecode)\n",
      "  Obtaining dependency information for sortedcontainers<3.0.0,>=2.1.0 from https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
      "Downloading hypothesis-6.136.2-py3-none-any.whl (524 kB)\n",
      "   ---------------------------------------- 0.0/524.2 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 358.4/524.2 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 524.2/524.2 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, pygtrie, attrs, hypothesis, pyctcdecode\n",
      "Successfully installed attrs-25.3.0 hypothesis-6.136.2 pyctcdecode-0.5.0 pygtrie-2.5.0 sortedcontainers-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8691ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 35.9 kB 1.7 MB/s 0:00:00\n",
      "     - 203.2 kB 3.1 MB/s 0:00:00\n",
      "     \\ 553.6 kB 5.0 MB/s 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml): started\n",
      "  Building wheel for kenlm (pyproject.toml): finished with status 'error'\n",
      "Failed to build kenlm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Building wheel for kenlm (pyproject.toml) did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [73 lines of output]\n",
      "      Will build with KenLM max_order set to 6\n",
      "      <3>WSL (30 - Relay) ERROR: CreateProcessCommon:735: execvpe(/bin/bash) failed: No such file or directory\n",
      "      <3>WSL (33 - Relay) ERROR: CreateProcessCommon:735: execvpe(/bin/bash) failed: No such file or directory\n",
      "      <3>WSL (36 - Relay) ERROR: CreateProcessCommon:735: execvpe(/bin/bash) failed: No such file or directory\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      -- Building for: NMake Makefiles\n",
      "      CMake Error at CMakeLists.txt:14 (project):\n",
      "        Generator\n",
      "      \n",
      "          NMake Makefiles\n",
      "      \n",
      "        does not support platform specification, but platform\n",
      "      \n",
      "          x64\n",
      "      \n",
      "        was specified.\n",
      "      \n",
      "      \n",
      "      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n",
      "      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n",
      "      -- Configuring incomplete, errors occurred!\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\User\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\User\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"C:\\Users\\User\\.julia\\conda\\3\\x86_64\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 251, in build_wheel\n",
      "          return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 435, in build_wheel\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 423, in _build\n",
      "          return self._build_with_temp_dir(\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 404, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 124, in <module>\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 115, in setup\n",
      "          return distutils.core.setup(**attrs)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 186, in setup\n",
      "          return run_commands(dist)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 202, in run_commands\n",
      "          dist.run_commands()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1002, in run_commands\n",
      "          self.run_command(cmd)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1102, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\command\\bdist_wheel.py\", line 370, in run\n",
      "          self.run_command(\"build\")\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 357, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1102, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 135, in run\n",
      "          self.run_command(cmd_name)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 357, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1102, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-env-276n70as\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 1021, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"<string>\", line 104, in run\n",
      "        File \"C:\\Users\\User\\.julia\\conda\\3\\x86_64\\lib\\subprocess.py\", line 369, in check_call\n",
      "          raise CalledProcessError(retcode, cmd)\n",
      "      subprocess.CalledProcessError: Command '['cmake', 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-86ylc9bt', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-86ylc9bt\\\\build\\\\lib.win-amd64-cpython-310', '-DBUILD_SHARED_LIBS=ON', '-DBUILD_PYTHON_STANDALONE=ON', '-DKENLM_MAX_ORDER=6', '-DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON', '-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-86ylc9bt\\\\build\\\\lib.win-amd64-cpython-310', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-86ylc9bt\\\\build\\\\lib.win-amd64-cpython-310', '-DCMAKE_ARCHIVE_OUTPUT_DIRECTORY_RELEASE=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-86ylc9bt\\\\build\\\\lib.win-amd64-cpython-310', '-A', 'x64']' returned non-zero exit status 1.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for kenlm\n",
      "ERROR: Could not build wheels for kenlm, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a6c388f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import BeamSearchDecoderCTC, Alphabet\n",
    "\n",
    "labels = [\"<pad>\"] + list(\"abcdefghijklmnopqrstuvwxyz '\")  # 29 labels total\n",
    "alphabet = Alphabet.build_alphabet(labels)  # now index 0 is blank\n",
    "decoder = BeamSearchDecoderCTC(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b4d2",
   "metadata": {},
   "source": [
    "### ≈°to je dropout, za≈°to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e2ffb",
   "metadata": {},
   "source": [
    "promjena max poola na (2,1)\n",
    " Why we also changed stride=(2, 1)?\n",
    "We usually want stride = kernel size in pooling.\n",
    "\n",
    "This avoids overlapping windows and makes the output size predictable.\n",
    "\n",
    "If you used:\n",
    "\n",
    "python\n",
    "Kopiraj\n",
    "Uredi\n",
    "kernel_size=(2, 2), stride=(2, 1)\n",
    "You‚Äôd pool in time ‚Äî but only move forward 1 step\n",
    "\n",
    "That would result in overlapping pooling in time ‚Äî more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_INPUT_LENGTH = 8000  # or another value that fits your dataset\n",
    "FIXED_TIME_STEPS = 1024  # Adjust this value based on your MelSpectrogram output and model\n",
    "class CNN_LSTM_Model(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=2, output_size=len(labels)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # ‚ö†Ô∏è Dummy input to infer shape ovako mozemo kasnije promijeniti\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 40, FIXED_TIME_STEPS)  # [B, C, Freq, Time]\n",
    "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy))))\n",
    "            x = torch.relu(self.bn2(self.conv2(x)))\n",
    "            _, channels, freq, _ = x.shape\n",
    "            lstm_input_size = channels * freq\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        #print(\"Input to conv1:\", x.shape)\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        #print(\"After pool1:\", x.shape)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        #print(\"After conv2:\", x.shape)\n",
    "        batch, channels, freq, time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).contiguous().view(batch, time, channels * freq)\n",
    "        #print(\"After reshape for LSTM:\", x.shape)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.dropout(self.fc(lstm_out))\n",
    "        x = x.permute(1, 0, 2)  # [time, batch, classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2637efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CTC Loss function\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea633bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset=\"train-clean-100\", transform=None, max_duration=10.0):\n",
    "        self.root_dir = root_dir\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        max_chars = 20  # You can lower to 30 for overfitting tests\n",
    "\n",
    "        subset_dir = os.path.join(self.root_dir, subset)\n",
    "        for speaker in os.listdir(subset_dir):\n",
    "            speaker_path = os.path.join(subset_dir, speaker)\n",
    "            if os.path.isdir(speaker_path):\n",
    "                for chapter in os.listdir(speaker_path):\n",
    "                    chapter_path = os.path.join(speaker_path, chapter)\n",
    "                    if os.path.isdir(chapter_path):\n",
    "                        trans_file = os.path.join(chapter_path, f\"{speaker}-{chapter}.trans.txt\")\n",
    "                        audio_files = [f for f in os.listdir(chapter_path) if f.endswith('.flac')]\n",
    "                        \n",
    "                        # Load all transcriptions\n",
    "                        trans_dict = {}\n",
    "                        with open(trans_file, 'r') as f:\n",
    "                            for line in f:\n",
    "                                parts = line.strip().split(' ', 1)\n",
    "                                if len(parts) == 2:\n",
    "                                    trans_dict[parts[0]] = parts[1].lower()\n",
    "\n",
    "                        for audio_file in audio_files:\n",
    "                            audio_path = os.path.join(chapter_path, audio_file)\n",
    "                            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                            duration = waveform.shape[1] / sample_rate\n",
    "                            utt_id = audio_file.replace('.flac', '')\n",
    "                            transcription = trans_dict.get(utt_id, \"\")\n",
    "                            if duration <= max_duration and len(transcription) <= max_chars:\n",
    "                                self.data.append({\n",
    "                                    'audio': audio_path,\n",
    "                                    'transcription': transcription\n",
    "                                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        waveform, sample_rate = torchaudio.load(sample['audio'])\n",
    "        waveform = waveform.squeeze(0)\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(waveform)\n",
    "            spectrogram = spectrogram.unsqueeze(0)\n",
    "            return spectrogram, sample['transcription']\n",
    "        return waveform.unsqueeze(0), sample['transcription']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e624d5",
   "metadata": {},
   "source": [
    "FIXED_TIME_STEPS = 512, poveƒáao s 128, jer onda je bilo pre malo output character predictiona, a po≈°to sam poveƒáao moguƒá input da pro≈°irim dataset\n",
    "onda du≈æe reƒçenice fiziƒçki nisu mogle biti reprezentirane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16ec8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "FIXED_TIME_STEPS = FIXED_TIME_STEPS\n",
    "\n",
    "def pad_waveform(waveform, target_length=FIXED_INPUT_LENGTH):\n",
    "    # waveform shape: [channels, time] or [1, time]\n",
    "    waveform_length = waveform.size(1)\n",
    "    if waveform_length < target_length:\n",
    "        padding = target_length - waveform_length\n",
    "        waveform = F.pad(waveform, (0, padding))\n",
    "    elif waveform_length > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def resize_spec(spec, target_width=FIXED_TIME_STEPS):\n",
    "    current_width = spec.shape[-1]\n",
    "    if current_width < target_width:\n",
    "        return F.pad(spec, (0, target_width - current_width))\n",
    "    else:\n",
    "        return spec[..., :target_width]\n",
    "def collate_fn(batch):\n",
    "    spectrograms = [resize_spec(item[0]) for item in batch]\n",
    "    transcripts = [item[1] for item in batch]\n",
    "\n",
    "    spectrograms = torch.stack(spectrograms)  # shape: [B, 1, 40, FIXED_TIME_STEPS]\n",
    "\n",
    "    # Targets\n",
    "    targets = []\n",
    "    target_lengths = []\n",
    "    for t in transcripts:\n",
    "        int_seq = text_to_int_sequence(t)\n",
    "        targets.extend(int_seq)\n",
    "        target_lengths.append(len(int_seq))\n",
    "\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "\n",
    "    return spectrograms, targets, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d11e3",
   "metadata": {},
   "source": [
    "## DONT FORGET TO RELOAD DATALOADER IF YOU FILTER FOR LENGTH ILI NE≈†TO DRUGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c5e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeechDataset(\n",
    "    root_dir=r\"X:\\AIx\\PROJECTS\\voiceToText\\mojModel\\data\\LibriSpeech\",\n",
    "    subset=\"train-clean-100\",\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3dc85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered samples: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total filtered samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d10674a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2444ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CNN_LSTM_Model(hidden_size=128, output_size=40)\n",
    "    # Run one batch through model to initialize LSTM/FC\n",
    "#inputs, _, _ = next(iter(train_loader))\n",
    "#_ = model(inputs.to(torch.float32))\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "426738cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b216b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before model: torch.Size([2, 1, 40, 1024])\n"
     ]
    }
   ],
   "source": [
    "model = CNN_LSTM_Model(hidden_size=64, num_layers=1, output_size=output_size).to(device)\n",
    "# Run one batch through model to initialize LSTM/FC\n",
    "inputs, _, _ = next(iter(train_loader))\n",
    "print(\"Shape before model:\", inputs.shape)  # should be [B, 1, 40, T]\n",
    "_ = model(inputs.to(torch.float32).to(device))\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # ovo je za overfitting test\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb521b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbcf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_char_map = {v: k for k, v in char_map.items()}\n",
    "\n",
    "def decode_prediction(pred):\n",
    "    decoded = []\n",
    "    prev = -1\n",
    "    for p in pred:\n",
    "        if p != prev and p != 0:  # Skip blank (0)\n",
    "            decoded.append(labels[p])\n",
    "        prev = p\n",
    "    return ''.join(decoded)\n",
    "\n",
    "def decode_target(target_seq):\n",
    "    return ''.join(inv_char_map.get(t, '?') for t in target_seq if t != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c267671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d6717",
   "metadata": {},
   "source": [
    "treba fine tuneat, ako je target_length veƒái od output_time_steps ni dobro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea8150",
   "metadata": {},
   "source": [
    "napravit cu i ovo\n",
    "\n",
    "self.pool = nn.MaxPool2d((2, 2))  # ‚õî reduces both time & freq\n",
    "This halves time resolution.\n",
    "\n",
    "‚û°Ô∏è Change it to:\n",
    "\n",
    "self.pool = nn.MaxPool2d((2, 1))  # ‚úÖ only reduces frequency, keeps time\n",
    "This preserves more time steps ‚Üí longer output.\n",
    "\n",
    "üîß Option C: Combine both (best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5a52cbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepoch_losses = []  # stores average loss per epoch\\nepoch_cers = [] # stores average CER per epoch\\nfor epoch in range(80):\\n    running_loss = 0.0\\n    cer_scores = []\\n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\\n        inputs, targets, target_lengths = batch\\n        inputs = inputs.to(torch.float32).to(device)\\n        targets = targets.to(device)\\n        target_lengths = target_lengths.to(device)\\n        optimizer.zero_grad()\\n\\n        outputs = model(inputs)\\n        # da vidimo output koliki je a koliki treba biti\\n        print(\"Output shape:\", outputs.shape)  # shape: [time, batch, vocab_size]\\n        print(\"Target lengths:\", target_lengths.tolist())  # List of target lengths\\n\\n\\n        # Decode predictions and targets\\n        preds = outputs.argmax(-1).T  # shape: [batch, time]\\n        target_offset = 0\\n        for i in range(preds.shape[0]):\\n            pred_str = decode_prediction(preds[i].tolist())\\n            true_len = target_lengths[i].item()\\n            target_str = decode_target(targets[target_offset:target_offset + true_len].tolist())\\n            target_offset += true_len\\n            cer_score = cer(target_str, pred_str)\\n            cer_scores.append(cer_score)\\n            if i < 1:  # Only print one sample per batch for readability\\n                print(f\"\\nWanted:    {target_str}\")\\n                print(f\"Predicted: {pred_str if pred_str else \\'[BLANK]\\'}\")\\n                print(f\"CER:       {cer_score:.2f}\")\\n\\n\\n        input_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long, device=device)\\n        loss = ctc_loss(outputs, targets, input_lengths, target_lengths)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        tqdm.write(f\"Epoch {epoch+1} Batch {batch_idx+1} Loss: {loss.item():.4f}\")\\n    average_loss = running_loss / (batch_idx + 1)\\n    epoch_losses.append(average_loss)\\n    average_cer = sum(cer_scores) / len(cer_scores)\\n    epoch_cers.append(average_cer)\\n\\n    clear_output(wait=True)\\n    plt.figure(figsize=(12, 4))\\n\\n    plt.subplot(1, 2, 1)\\n    plt.plot(epoch_losses, marker=\\'o\\')\\n    plt.title(\"CTC Loss Over Epochs\")\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Average Loss\")\\n    plt.grid(True)\\n    plt.ticklabel_format(useOffset=False, style=\\'plain\\', axis=\\'y\\')\\n\\n    plt.subplot(1, 2, 2)\\n    plt.plot(epoch_cers, marker=\\'x\\', color=\\'orange\\')\\n    plt.title(\"CER Over Epochs\")\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Character Error Rate\")\\n    plt.grid(True)\\n    plt.ticklabel_format(useOffset=False, style=\\'plain\\', axis=\\'y\\')\\n\\n\\n    plt.tight_layout()\\n    plt.show()\\n\\n\\n    print(f\"Epoch {epoch + 1} Average Loss: {average_loss:.4f}\")\\n    print(f\"Epoch {epoch + 1} Average CER:  {average_cer:.4f}\")\\n\\n    scheduler.step()\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\"\"\"\n",
    "epoch_losses = []  # stores average loss per epoch\n",
    "epoch_cers = [] # stores average CER per epoch\n",
    "for epoch in range(80):\n",
    "    running_loss = 0.0\n",
    "    cer_scores = []\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        inputs, targets, target_lengths = batch\n",
    "        inputs = inputs.to(torch.float32).to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        # da vidimo output koliki je a koliki treba biti\n",
    "        print(\"Output shape:\", outputs.shape)  # shape: [time, batch, vocab_size]\n",
    "        print(\"Target lengths:\", target_lengths.tolist())  # List of target lengths\n",
    "\n",
    "\n",
    "        # Decode predictions and targets\n",
    "        preds = outputs.argmax(-1).T  # shape: [batch, time]\n",
    "        target_offset = 0\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_str = decode_prediction(preds[i].tolist())\n",
    "            true_len = target_lengths[i].item()\n",
    "            target_str = decode_target(targets[target_offset:target_offset + true_len].tolist())\n",
    "            target_offset += true_len\n",
    "            cer_score = cer(target_str, pred_str)\n",
    "            cer_scores.append(cer_score)\n",
    "            if i < 1:  # Only print one sample per batch for readability\n",
    "                print(f\"\\nWanted:    {target_str}\")\n",
    "                print(f\"Predicted: {pred_str if pred_str else '[BLANK]'}\")\n",
    "                print(f\"CER:       {cer_score:.2f}\")\n",
    "\n",
    "\n",
    "        input_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long, device=device)\n",
    "        loss = ctc_loss(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        tqdm.write(f\"Epoch {epoch+1} Batch {batch_idx+1} Loss: {loss.item():.4f}\")\n",
    "    average_loss = running_loss / (batch_idx + 1)\n",
    "    epoch_losses.append(average_loss)\n",
    "    average_cer = sum(cer_scores) / len(cer_scores)\n",
    "    epoch_cers.append(average_cer)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_losses, marker='o')\n",
    "    plt.title(\"CTC Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='y')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_cers, marker='x', color='orange')\n",
    "    plt.title(\"CER Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Character Error Rate\")\n",
    "    plt.grid(True)\n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='y')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Average CER:  {average_cer:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9abb43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Target indices: [14, 15, 18, 20, 8, 1, 14, 7, 5, 18, 27, 1, 2, 2, 5, 25]\n",
      "[DEBUG] Decoded target: northanger abbey\n"
     ]
    }
   ],
   "source": [
    "# Get a fresh iterator\n",
    "debug_batch = next(iter(train_loader))\n",
    "\n",
    "# Use only the first sample from the batch\n",
    "inputs = debug_batch[0][0].unsqueeze(0)              # shape: [1, 1, 40, T]\n",
    "target_len = debug_batch[2][0].item()\n",
    "targets = debug_batch[1][:target_len]                # slice target from flattened array\n",
    "target_lengths = debug_batch[2][0].unsqueeze(0)      # shape: [1]\n",
    "\n",
    "debug_batch = [inputs, targets, target_lengths]\n",
    "\n",
    "# Print decoded target to confirm\n",
    "print(\"[DEBUG] Target indices:\", targets.tolist())\n",
    "print(\"[DEBUG] Decoded target:\", decode_target(targets.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e18256bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Target length: 16\n",
      "[DEBUG] Target indices: [14, 15, 18, 20, 8, 1, 14, 7, 5, 18, 27, 1, 2, 2, 5, 25]\n",
      "[DEBUG] Decoded target: northanger abbey\n"
     ]
    }
   ],
   "source": [
    "print(\"[DEBUG] Target length:\", target_lengths.item())\n",
    "print(\"[DEBUG] Target indices:\", targets.tolist())\n",
    "print(\"[DEBUG] Decoded target:\", decode_target(targets.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4aeed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model():\n",
    "    model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)\n",
    "\n",
    "model = CNN_LSTM_Model(hidden_size=64, num_layers=1, output_size=output_size).to(device)\n",
    "reset_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "349c70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Step 1\n",
      "Target:    northanger abbey\n",
      "Greedy:    htg gegiug owhwowoso o x ogow'a'tg'tob' 'a'xag t gx g\n",
      "Beam:      tgtgtgtgtgtgtgtgtgtgtgegtig whwxouo o oxa't'tob' ' 'a'x g x tgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgtgt\n",
      "[DEBUG] CER: 31.1875\n",
      "\n",
      "[DEBUG] Step 51\n",
      "Target:    northanger abbey\n",
      "Greedy:    g\n",
      "Beam:      g\n",
      "[DEBUG] CER: 0.9375\n",
      "\n",
      "[DEBUG] Step 101\n",
      "Target:    northanger abbey\n",
      "Greedy:    g\n",
      "Beam:      gng\n",
      "[DEBUG] CER: 0.8750\n",
      "\n",
      "[DEBUG] Step 151\n",
      "Target:    northanger abbey\n",
      "Greedy:    ng\n",
      "Beam:      nrng\n",
      "[DEBUG] CER: 0.7500\n",
      "\n",
      "[DEBUG] Step 201\n",
      "Target:    northanger abbey\n",
      "Greedy:    norhng\n",
      "Beam:      norhang\n",
      "[DEBUG] CER: 0.5625\n",
      "\n",
      "[DEBUG] Step 251\n",
      "Target:    northanger abbey\n",
      "Greedy:    norhang\n",
      "Beam:      norhang\n",
      "[DEBUG] CER: 0.5625\n",
      "\n",
      "[DEBUG] Step 301\n",
      "Target:    northanger abbey\n",
      "Greedy:    norhang\n",
      "Beam:      norhang\n",
      "[DEBUG] CER: 0.5625\n",
      "\n",
      "[DEBUG] Step 351\n",
      "Target:    northanger abbey\n",
      "Greedy:    northang\n",
      "Beam:      northang\n",
      "[DEBUG] CER: 0.5000\n",
      "\n",
      "[DEBUG] Step 401\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 451\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 501\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 551\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 601\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 651\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 701\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 751\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 801\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 851\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 901\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 951\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1001\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1051\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1101\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1151\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1201\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1251\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1301\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1351\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1401\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1451\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1501\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1551\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1601\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1651\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1701\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1751\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1801\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1851\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1901\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 1951\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2001\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2051\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2101\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2151\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2201\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2251\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2301\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2351\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2401\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2451\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2501\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2551\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2601\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2651\n",
      "Target:    northanger abbey\n",
      "Greedy:    northngy\n",
      "Beam:      northngy\n",
      "[DEBUG] CER: 0.5000\n",
      "\n",
      "[DEBUG] Step 2701\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2751\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2801\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2851\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2901\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n",
      "\n",
      "[DEBUG] Step 2951\n",
      "Target:    northanger abbey\n",
      "Greedy:    northangy\n",
      "Beam:      northangy\n",
      "[DEBUG] CER: 0.4375\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(3000):\n",
    "    inputs, targets, target_lengths = debug_batch\n",
    "    inputs = inputs.to(torch.float32).to(device)\n",
    "    targets = targets.to(device)\n",
    "    target_lengths = target_lengths.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)\n",
    "\n",
    "    #print(\"[DEBUG] log_probs shape:\", log_probs.shape)\n",
    "\n",
    "    input_lengths = torch.full((inputs.size(0),), log_probs.size(0), dtype=torch.long, device=device)\n",
    "    loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        log_probs_np = log_probs.detach().cpu().numpy()  # [T, B, C]\n",
    "        decoded_str = decoder.decode(log_probs_np[:, 0])  # beam\n",
    "\n",
    "        greedy_indices = log_probs.argmax(-1).T[0].tolist()\n",
    "        greedy_str = decode_prediction(greedy_indices)\n",
    "\n",
    "        true_len = target_lengths[0].item()\n",
    "        target_str = decode_target(targets[:true_len].tolist())\n",
    "\n",
    "        print(f\"\\n[DEBUG] Step {step+1}\")\n",
    "        print(f\"Target:    {target_str}\")\n",
    "        print(f\"Greedy:    {greedy_str}\")\n",
    "        print(f\"Beam:      {decoded_str}\")\n",
    "        cer_score = cer(target_str, decoded_str)\n",
    "        print(f\"[DEBUG] CER: {cer_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
