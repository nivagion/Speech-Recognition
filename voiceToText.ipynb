{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d054b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fb9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651d71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9bcb0",
   "metadata": {},
   "source": [
    "##todo:\n",
    "splitat u validation i u train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ab8b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "1\n",
      "NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd5ae47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transform (Mel spectrogram transformation)\n",
    "transform = MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=400,        # 25 ms window\n",
    "    hop_length=160,   # 10 ms stride\n",
    "    n_mels=40\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905dac8",
   "metadata": {},
   "source": [
    "ako treba If you still need nn.Sequential later, wrap it like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21c418",
   "metadata": {},
   "source": [
    "transform = nn.Sequential(\n",
    "    MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c99ed92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simple character map (extend as needed)\n",
    "char_map = {c: i+1 for i, c in enumerate(\"abcdefghijklmnopqrstuvwxyz '\")}\n",
    "char_map['<blank>'] = 0\n",
    "output_size = len(char_map)\n",
    "def text_to_int_sequence(text):\n",
    "    return [char_map.get(c, 0) for c in text.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585b4d2",
   "metadata": {},
   "source": [
    "### što je dropout, zašto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e2ffb",
   "metadata": {},
   "source": [
    "promjena max poola na (2,1)\n",
    " Why we also changed stride=(2, 1)?\n",
    "We usually want stride = kernel size in pooling.\n",
    "\n",
    "This avoids overlapping windows and makes the output size predictable.\n",
    "\n",
    "If you used:\n",
    "\n",
    "python\n",
    "Kopiraj\n",
    "Uredi\n",
    "kernel_size=(2, 2), stride=(2, 1)\n",
    "You’d pool in time — but only move forward 1 step\n",
    "\n",
    "That would result in overlapping pooling in time — more expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d995e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_INPUT_LENGTH = 8000  # or another value that fits your dataset\n",
    "FIXED_TIME_STEPS = 1024  # Adjust this value based on your MelSpectrogram output and model\n",
    "class CNN_LSTM_Model(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=2, output_size=len(char_map)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # ⚠️ Dummy input to infer shape ovako mozemo kasnije promijeniti\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 40, FIXED_TIME_STEPS)  # [B, C, Freq, Time]\n",
    "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy))))\n",
    "            x = torch.relu(self.bn2(self.conv2(x)))\n",
    "            _, channels, freq, _ = x.shape\n",
    "            lstm_input_size = channels * freq\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        #print(\"Input to conv1:\", x.shape)\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        #print(\"After pool1:\", x.shape)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        #print(\"After conv2:\", x.shape)\n",
    "        batch, channels, freq, time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).contiguous().view(batch, time, channels * freq)\n",
    "        #print(\"After reshape for LSTM:\", x.shape)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.dropout(self.fc(lstm_out))\n",
    "        x = x.permute(1, 0, 2)  # [time, batch, classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2637efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CTC Loss function\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aea633bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset=\"train-clean-100\", transform=None, max_duration=10.0):\n",
    "        self.root_dir = root_dir\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        max_chars = 20  # You can lower to 30 for overfitting tests\n",
    "\n",
    "        subset_dir = os.path.join(self.root_dir, subset)\n",
    "        for speaker in os.listdir(subset_dir):\n",
    "            speaker_path = os.path.join(subset_dir, speaker)\n",
    "            if os.path.isdir(speaker_path):\n",
    "                for chapter in os.listdir(speaker_path):\n",
    "                    chapter_path = os.path.join(speaker_path, chapter)\n",
    "                    if os.path.isdir(chapter_path):\n",
    "                        trans_file = os.path.join(chapter_path, f\"{speaker}-{chapter}.trans.txt\")\n",
    "                        audio_files = [f for f in os.listdir(chapter_path) if f.endswith('.flac')]\n",
    "                        \n",
    "                        # Load all transcriptions\n",
    "                        trans_dict = {}\n",
    "                        with open(trans_file, 'r') as f:\n",
    "                            for line in f:\n",
    "                                parts = line.strip().split(' ', 1)\n",
    "                                if len(parts) == 2:\n",
    "                                    trans_dict[parts[0]] = parts[1].lower()\n",
    "\n",
    "                        for audio_file in audio_files:\n",
    "                            audio_path = os.path.join(chapter_path, audio_file)\n",
    "                            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                            duration = waveform.shape[1] / sample_rate\n",
    "                            utt_id = audio_file.replace('.flac', '')\n",
    "                            transcription = trans_dict.get(utt_id, \"\")\n",
    "                            if duration <= max_duration and len(transcription) <= max_chars:\n",
    "                                self.data.append({\n",
    "                                    'audio': audio_path,\n",
    "                                    'transcription': transcription\n",
    "                                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        waveform, sample_rate = torchaudio.load(sample['audio'])\n",
    "        waveform = waveform.squeeze(0)\n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(waveform)\n",
    "            spectrogram = spectrogram.unsqueeze(0)\n",
    "            return spectrogram, sample['transcription']\n",
    "        return waveform.unsqueeze(0), sample['transcription']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e624d5",
   "metadata": {},
   "source": [
    "FIXED_TIME_STEPS = 512, povećao s 128, jer onda je bilo pre malo output character predictiona, a pošto sam povećao moguć input da proširim dataset\n",
    "onda duže rečenice fizički nisu mogle biti reprezentirane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16ec8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "FIXED_TIME_STEPS = FIXED_TIME_STEPS\n",
    "\n",
    "def pad_waveform(waveform, target_length=FIXED_INPUT_LENGTH):\n",
    "    # waveform shape: [channels, time] or [1, time]\n",
    "    waveform_length = waveform.size(1)\n",
    "    if waveform_length < target_length:\n",
    "        padding = target_length - waveform_length\n",
    "        waveform = F.pad(waveform, (0, padding))\n",
    "    elif waveform_length > target_length:\n",
    "        waveform = waveform[:, :target_length]\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def resize_spec(spec, target_width=FIXED_TIME_STEPS):\n",
    "    current_width = spec.shape[-1]\n",
    "    if current_width < target_width:\n",
    "        return F.pad(spec, (0, target_width - current_width))\n",
    "    else:\n",
    "        return spec[..., :target_width]\n",
    "def collate_fn(batch):\n",
    "    spectrograms = [resize_spec(item[0]) for item in batch]\n",
    "    transcripts = [item[1] for item in batch]\n",
    "\n",
    "    spectrograms = torch.stack(spectrograms)  # shape: [B, 1, 40, FIXED_TIME_STEPS]\n",
    "\n",
    "    # Targets\n",
    "    targets = []\n",
    "    target_lengths = []\n",
    "    for t in transcripts:\n",
    "        int_seq = text_to_int_sequence(t)\n",
    "        targets.extend(int_seq)\n",
    "        target_lengths.append(len(int_seq))\n",
    "\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "\n",
    "    return spectrograms, targets, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d11e3",
   "metadata": {},
   "source": [
    "## DONT FORGET TO RELOAD DATALOADER IF YOU FILTER FOR LENGTH ILI NEŠTO DRUGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c5e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeechDataset(\n",
    "    root_dir=r\"X:\\AIx\\PROJECTS\\voiceToText\\mojModel\\data\\LibriSpeech\",\n",
    "    subset=\"train-clean-100\",\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3dc85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered samples: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total filtered samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d10674a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2444ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CNN_LSTM_Model(hidden_size=128, output_size=40)\n",
    "    # Run one batch through model to initialize LSTM/FC\n",
    "#inputs, _, _ = next(iter(train_loader))\n",
    "#_ = model(inputs.to(torch.float32))\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "426738cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b216b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before model: torch.Size([2, 1, 40, 1024])\n"
     ]
    }
   ],
   "source": [
    "model = CNN_LSTM_Model(hidden_size=64, num_layers=1, output_size=output_size).to(device)\n",
    "# Run one batch through model to initialize LSTM/FC\n",
    "inputs, _, _ = next(iter(train_loader))\n",
    "print(\"Shape before model:\", inputs.shape)  # should be [B, 1, 40, T]\n",
    "_ = model(inputs.to(torch.float32).to(device))\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2) # ovo je za overfitting test\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb521b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0bbcf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_char_map = {v: k for k, v in char_map.items()}\n",
    "\n",
    "def decode_prediction(pred):\n",
    "    # Collapse repeated characters and remove blanks (0)\n",
    "    decoded = []\n",
    "    prev = -1\n",
    "    for p in pred:\n",
    "        if p != prev and p != 0:\n",
    "            decoded.append(inv_char_map.get(p, '?'))\n",
    "        prev = p\n",
    "    return ''.join(decoded)\n",
    "\n",
    "def decode_target(target_seq):\n",
    "    return ''.join(inv_char_map.get(t, '?') for t in target_seq if t != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c267671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.julia\\conda\\3\\x86_64\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d6717",
   "metadata": {},
   "source": [
    "treba fine tuneat, ako je target_length veći od output_time_steps ni dobro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea8150",
   "metadata": {},
   "source": [
    "napravit cu i ovo\n",
    "\n",
    "self.pool = nn.MaxPool2d((2, 2))  # ⛔ reduces both time & freq\n",
    "This halves time resolution.\n",
    "\n",
    "➡️ Change it to:\n",
    "\n",
    "self.pool = nn.MaxPool2d((2, 1))  # ✅ only reduces frequency, keeps time\n",
    "This preserves more time steps → longer output.\n",
    "\n",
    "🔧 Option C: Combine both (best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a52cbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nepoch_losses = []  # stores average loss per epoch\\nepoch_cers = [] # stores average CER per epoch\\nfor epoch in range(80):\\n    running_loss = 0.0\\n    cer_scores = []\\n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\\n        inputs, targets, target_lengths = batch\\n        inputs = inputs.to(torch.float32).to(device)\\n        targets = targets.to(device)\\n        target_lengths = target_lengths.to(device)\\n        optimizer.zero_grad()\\n\\n        outputs = model(inputs)\\n        # da vidimo output koliki je a koliki treba biti\\n        print(\"Output shape:\", outputs.shape)  # shape: [time, batch, vocab_size]\\n        print(\"Target lengths:\", target_lengths.tolist())  # List of target lengths\\n\\n\\n        # Decode predictions and targets\\n        preds = outputs.argmax(-1).T  # shape: [batch, time]\\n        target_offset = 0\\n        for i in range(preds.shape[0]):\\n            pred_str = decode_prediction(preds[i].tolist())\\n            true_len = target_lengths[i].item()\\n            target_str = decode_target(targets[target_offset:target_offset + true_len].tolist())\\n            target_offset += true_len\\n            cer_score = cer(target_str, pred_str)\\n            cer_scores.append(cer_score)\\n            if i < 1:  # Only print one sample per batch for readability\\n                print(f\"\\nWanted:    {target_str}\")\\n                print(f\"Predicted: {pred_str if pred_str else \\'[BLANK]\\'}\")\\n                print(f\"CER:       {cer_score:.2f}\")\\n\\n\\n        input_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long, device=device)\\n        loss = ctc_loss(outputs, targets, input_lengths, target_lengths)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        tqdm.write(f\"Epoch {epoch+1} Batch {batch_idx+1} Loss: {loss.item():.4f}\")\\n    average_loss = running_loss / (batch_idx + 1)\\n    epoch_losses.append(average_loss)\\n    average_cer = sum(cer_scores) / len(cer_scores)\\n    epoch_cers.append(average_cer)\\n\\n    clear_output(wait=True)\\n    plt.figure(figsize=(12, 4))\\n\\n    plt.subplot(1, 2, 1)\\n    plt.plot(epoch_losses, marker=\\'o\\')\\n    plt.title(\"CTC Loss Over Epochs\")\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Average Loss\")\\n    plt.grid(True)\\n    plt.ticklabel_format(useOffset=False, style=\\'plain\\', axis=\\'y\\')\\n\\n    plt.subplot(1, 2, 2)\\n    plt.plot(epoch_cers, marker=\\'x\\', color=\\'orange\\')\\n    plt.title(\"CER Over Epochs\")\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Character Error Rate\")\\n    plt.grid(True)\\n    plt.ticklabel_format(useOffset=False, style=\\'plain\\', axis=\\'y\\')\\n\\n\\n    plt.tight_layout()\\n    plt.show()\\n\\n\\n    print(f\"Epoch {epoch + 1} Average Loss: {average_loss:.4f}\")\\n    print(f\"Epoch {epoch + 1} Average CER:  {average_cer:.4f}\")\\n\\n    scheduler.step()\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from jiwer import wer, cer\n",
    "\n",
    "\"\"\"\n",
    "epoch_losses = []  # stores average loss per epoch\n",
    "epoch_cers = [] # stores average CER per epoch\n",
    "for epoch in range(80):\n",
    "    running_loss = 0.0\n",
    "    cer_scores = []\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        inputs, targets, target_lengths = batch\n",
    "        inputs = inputs.to(torch.float32).to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        # da vidimo output koliki je a koliki treba biti\n",
    "        print(\"Output shape:\", outputs.shape)  # shape: [time, batch, vocab_size]\n",
    "        print(\"Target lengths:\", target_lengths.tolist())  # List of target lengths\n",
    "\n",
    "\n",
    "        # Decode predictions and targets\n",
    "        preds = outputs.argmax(-1).T  # shape: [batch, time]\n",
    "        target_offset = 0\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_str = decode_prediction(preds[i].tolist())\n",
    "            true_len = target_lengths[i].item()\n",
    "            target_str = decode_target(targets[target_offset:target_offset + true_len].tolist())\n",
    "            target_offset += true_len\n",
    "            cer_score = cer(target_str, pred_str)\n",
    "            cer_scores.append(cer_score)\n",
    "            if i < 1:  # Only print one sample per batch for readability\n",
    "                print(f\"\\nWanted:    {target_str}\")\n",
    "                print(f\"Predicted: {pred_str if pred_str else '[BLANK]'}\")\n",
    "                print(f\"CER:       {cer_score:.2f}\")\n",
    "\n",
    "\n",
    "        input_lengths = torch.full(size=(inputs.size(0),), fill_value=outputs.size(0), dtype=torch.long, device=device)\n",
    "        loss = ctc_loss(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        tqdm.write(f\"Epoch {epoch+1} Batch {batch_idx+1} Loss: {loss.item():.4f}\")\n",
    "    average_loss = running_loss / (batch_idx + 1)\n",
    "    epoch_losses.append(average_loss)\n",
    "    average_cer = sum(cer_scores) / len(cer_scores)\n",
    "    epoch_cers.append(average_cer)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_losses, marker='o')\n",
    "    plt.title(\"CTC Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='y')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_cers, marker='x', color='orange')\n",
    "    plt.title(\"CER Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Character Error Rate\")\n",
    "    plt.grid(True)\n",
    "    plt.ticklabel_format(useOffset=False, style='plain', axis='y')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {average_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Average CER:  {average_cer:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74c17a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9abb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, target_lengths = debug_batch\n",
    "\n",
    "# Use the first sample\n",
    "inputs = inputs[0].unsqueeze(0)  # [1, 1, 40, T]\n",
    "first_len = target_lengths[0].item()\n",
    "targets = targets[:first_len]   # take the first `N` characters\n",
    "target_lengths = target_lengths[0].unsqueeze(0)  # shape: [1]\n",
    "\n",
    "# Reassign\n",
    "debug_batch = [inputs, targets, target_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e18256bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Target length: 19\n",
      "[DEBUG] Target indices: [19, 1, 9, 14, 20, 27, 2, 18, 9, 4, 5, 28, 19, 27, 5, 9, 7, 8, 20]\n",
      "[DEBUG] Decoded target: saint bride's eight\n"
     ]
    }
   ],
   "source": [
    "print(\"[DEBUG] Target length:\", target_lengths.item())\n",
    "print(\"[DEBUG] Target indices:\", targets.tolist())\n",
    "print(\"[DEBUG] Decoded target:\", decode_target(targets.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "349c70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Step 1\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 51\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 101\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 151\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 201\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 251\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 301\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 351\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 401\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 451\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 501\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 551\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 601\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 651\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 701\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 751\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 801\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 851\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 901\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 951\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1001\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1051\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1101\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1151\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1201\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1251\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1301\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1351\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1401\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1451\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1501\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1551\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1601\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1651\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1701\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1751\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1801\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1851\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1901\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 1951\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2001\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2051\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2101\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2151\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2201\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2251\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2301\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2351\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2401\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2451\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2501\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2551\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2601\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2651\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2701\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2751\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2801\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2851\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2901\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n",
      "\n",
      "[DEBUG] Step 2951\n",
      "Target:    saint bride's eight\n",
      "Predicted: saint bridt\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(3000):\n",
    "    inputs, targets, target_lengths = debug_batch\n",
    "    inputs = inputs.to(torch.float32).to(device)\n",
    "    targets = targets.to(device)\n",
    "    target_lengths = target_lengths.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #print(\"[DEBUG] Spectrogram mean:\", inputs.mean().item())\n",
    "    #print(\"[DEBUG] Spectrogram std: \", inputs.std().item())\n",
    "    outputs = model(inputs)\n",
    "    #print(\"[DEBUG] Output logits mean:\", outputs.mean().item())\n",
    "    #print(\"[DEBUG] Output logits std: \", outputs.std().item())\n",
    "    #print(\"[DEBUG] Output argmax (first 10 tokens of sample 0):\", outputs.argmax(-1)[:10, 0].tolist())\n",
    "    log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)\n",
    "    input_lengths = torch.full((inputs.size(0),), outputs.size(0), dtype=torch.long, device=device)\n",
    "    loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "    input_lengths = torch.full((inputs.size(0),), outputs.size(0), dtype=torch.long, device=device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #print(f\"[DEBUG] Step {step+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Decode prediction every N steps\n",
    "    if step % 50 == 0:\n",
    "        preds = outputs.argmax(-1).T  # [batch, time]\n",
    "        target_offset = 0\n",
    "        for i in range(preds.shape[0]):\n",
    "            pred_str = decode_prediction(preds[i].tolist())\n",
    "            true_len = target_lengths[i].item()\n",
    "            target_str = decode_target(targets[target_offset:target_offset + true_len].tolist())\n",
    "            target_offset += true_len\n",
    "            print(f\"\\n[DEBUG] Step {step+1}\")\n",
    "            print(f\"Target:    {target_str}\")\n",
    "            print(f\"Predicted: {pred_str if pred_str else '[BLANK]'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
